{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate filtering metrics for a dataset and store in a .hdf file.\n",
    "\"\"\"\n",
    "# MIT License\n",
    "# \n",
    "# Copyright (c) 2016 David Sandberg\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import facenet\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "import math\n",
    "from tensorflow.python.platform import gfile\n",
    "from six import iteritems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    dataset = facenet.get_dataset(args.dataset_dir)\n",
    "  \n",
    "    with tf.Graph().as_default():\n",
    "      \n",
    "        # Get a list of image paths and their labels\n",
    "        image_list, label_list = facenet.get_image_paths_and_labels(dataset)\n",
    "        nrof_images = len(image_list)\n",
    "        image_indices = range(nrof_images)\n",
    "\n",
    "        image_batch, label_batch = facenet.read_and_augment_data(image_list,\n",
    "            image_indices, args.image_size, args.batch_size, None, \n",
    "            False, False, False, nrof_preprocess_threads=4, shuffle=False)\n",
    "        \n",
    "        model_exp = os.path.expanduser(args.model_file)\n",
    "        with gfile.FastGFile(model_exp,'rb') as f:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            input_map={'input':image_batch, 'phase_train':False}\n",
    "            tf.import_graph_def(graph_def, input_map=input_map, name='net')\n",
    "        \n",
    "        embeddings = tf.get_default_graph().get_tensor_by_name(\"net/embeddings:0\")\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            tf.train.start_queue_runners(sess=sess)\n",
    "                \n",
    "            embedding_size = int(embeddings.get_shape()[1])\n",
    "            nrof_batches = int(math.ceil(nrof_images / args.batch_size))\n",
    "            nrof_classes = len(dataset)\n",
    "            label_array = np.array(label_list)\n",
    "            class_names = [cls.name for cls in dataset]\n",
    "            nrof_examples_per_class = [ len(cls.image_paths) for cls in dataset ]\n",
    "            class_variance = np.zeros((nrof_classes,))\n",
    "            class_center = np.zeros((nrof_classes,embedding_size))\n",
    "            distance_to_center = np.ones((len(label_list),))*np.NaN\n",
    "            emb_array = np.zeros((0,embedding_size))\n",
    "            idx_array = np.zeros((0,), dtype=np.int32)\n",
    "            lab_array = np.zeros((0,), dtype=np.int32)\n",
    "            index_arr = np.append(0, np.cumsum(nrof_examples_per_class))\n",
    "            for i in range(nrof_batches):\n",
    "                t = time.time()\n",
    "                emb, idx = sess.run([embeddings, label_batch])\n",
    "                emb_array = np.append(emb_array, emb, axis=0)\n",
    "                idx_array = np.append(idx_array, idx, axis=0)\n",
    "                lab_array = np.append(lab_array, label_array[idx], axis=0)\n",
    "                for cls in set(lab_array):\n",
    "                    cls_idx = np.where(lab_array==cls)[0]\n",
    "                    if cls_idx.shape[0]==nrof_examples_per_class[cls]:\n",
    "                        # We have calculated all the embeddings for this class\n",
    "                        i2 = np.argsort(idx_array[cls_idx])\n",
    "                        emb_class = emb_array[cls_idx,:]\n",
    "                        emb_sort = emb_class[i2,:]\n",
    "                        center = np.mean(emb_sort, axis=0)\n",
    "                        diffs = emb_sort - center\n",
    "                        dists_sqr = np.sum(np.square(diffs), axis=1)\n",
    "                        class_variance[cls] = np.mean(dists_sqr)\n",
    "                        class_center[cls,:] = center\n",
    "                        distance_to_center[index_arr[cls]:index_arr[cls+1]] = np.sqrt(dists_sqr)\n",
    "                        emb_array = np.delete(emb_array, cls_idx, axis=0)\n",
    "                        idx_array = np.delete(idx_array, cls_idx, axis=0)\n",
    "                        lab_array = np.delete(lab_array, cls_idx, axis=0)\n",
    "\n",
    "                        \n",
    "                print('Batch %d in %.3f seconds' % (i, time.time()-t))\n",
    "                \n",
    "            print('Writing filtering data to %s' % args.data_file_name)\n",
    "            mdict = {'class_names':class_names, 'image_list':image_list, 'label_list':label_list, 'distance_to_center':distance_to_center }\n",
    "            with h5py.File(args.data_file_name, 'w') as f:\n",
    "                for key, value in iteritems(mdict):\n",
    "                    f.create_dataset(key, data=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def parse_arguments(argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('dataset_dir', type=str,\n",
    "        help='Path to the directory containing aligned dataset.')\n",
    "    parser.add_argument('model_file', type=str,\n",
    "        help='File containing the frozen model in protobuf (.pb) format to use for feature extraction.')\n",
    "    parser.add_argument('data_file_name', type=str,\n",
    "        help='The name of the file to store filtering data in.')\n",
    "    parser.add_argument('--image_size', type=int,\n",
    "        help='Image size.', default=160)\n",
    "    parser.add_argument('--batch_size', type=int,\n",
    "        help='Number of images to process in a batch.', default=90)\n",
    "    return parser.parse_args(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(parse_arguments(sys.argv[1:]))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
