{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training a face recognizer with TensorFlow based on the FaceNet paper\n",
    "FaceNet: A Unified Embedding for Face Recognition and Clustering: http://arxiv.org/abs/1503.03832\n",
    "\"\"\"\n",
    "# MIT License\n",
    "# \n",
    "# Copyright (c) 2016 David Sandberg\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import importlib\n",
    "import itertools\n",
    "import argparse\n",
    "import facenet\n",
    "import lfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import data_flow_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from six.moves import xrange  # @UnresolvedImport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "  \n",
    "    network = importlib.import_module(args.model_def)\n",
    "\n",
    "    subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n",
    "    if not os.path.isdir(log_dir):  # Create the log directory if it doesn't exist\n",
    "        os.makedirs(log_dir)\n",
    "    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n",
    "    if not os.path.isdir(model_dir):  # Create the model directory if it doesn't exist\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    # Write arguments to a text file\n",
    "    facenet.write_arguments_to_file(args, os.path.join(log_dir, 'arguments.txt'))\n",
    "        \n",
    "    # Store some git revision info in a text file in the log directory\n",
    "    src_path,_ = os.path.split(os.path.realpath(__file__))\n",
    "    facenet.store_revision_info(src_path, log_dir, ' '.join(sys.argv))\n",
    "\n",
    "    np.random.seed(seed=args.seed)\n",
    "    train_set = facenet.get_dataset(args.data_dir)\n",
    "    \n",
    "    print('Model directory: %s' % model_dir)\n",
    "    print('Log directory: %s' % log_dir)\n",
    "    if args.pretrained_model:\n",
    "        print('Pre-trained model: %s' % os.path.expanduser(args.pretrained_model))\n",
    "    \n",
    "    if args.lfw_dir:\n",
    "        print('LFW directory: %s' % args.lfw_dir)\n",
    "        # Read the file containing the pairs used for testing\n",
    "        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n",
    "        # Get the paths for the corresponding images\n",
    "        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n",
    "        \n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        tf.set_random_seed(args.seed)\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        # Placeholder for the learning rate\n",
    "        learning_rate_placeholder = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        \n",
    "        batch_size_placeholder = tf.placeholder(tf.int32, name='batch_size')\n",
    "        \n",
    "        phase_train_placeholder = tf.placeholder(tf.bool, name='phase_train')\n",
    "        \n",
    "        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name='image_paths')\n",
    "        labels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name='labels')\n",
    "        \n",
    "        input_queue = data_flow_ops.FIFOQueue(capacity=100000,\n",
    "                                    dtypes=[tf.string, tf.int64],\n",
    "                                    shapes=[(3,), (3,)],\n",
    "                                    shared_name=None, name=None)\n",
    "        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder])\n",
    "        \n",
    "        nrof_preprocess_threads = 4\n",
    "        images_and_labels = []\n",
    "        for _ in range(nrof_preprocess_threads):\n",
    "            filenames, label = input_queue.dequeue()\n",
    "            images = []\n",
    "            for filename in tf.unstack(filenames):\n",
    "                file_contents = tf.read_file(filename)\n",
    "                image = tf.image.decode_image(file_contents, channels=3)\n",
    "                \n",
    "                if args.random_crop:\n",
    "                    image = tf.random_crop(image, [args.image_size, args.image_size, 3])\n",
    "                else:\n",
    "                    image = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size)\n",
    "                if args.random_flip:\n",
    "                    image = tf.image.random_flip_left_right(image)\n",
    "    \n",
    "                #pylint: disable=no-member\n",
    "                image.set_shape((args.image_size, args.image_size, 3))\n",
    "                images.append(tf.image.per_image_standardization(image))\n",
    "            images_and_labels.append([images, label])\n",
    "    \n",
    "        image_batch, labels_batch = tf.train.batch_join(\n",
    "            images_and_labels, batch_size=batch_size_placeholder, \n",
    "            shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,\n",
    "            capacity=4 * nrof_preprocess_threads * args.batch_size,\n",
    "            allow_smaller_final_batch=True)\n",
    "        image_batch = tf.identity(image_batch, 'image_batch')\n",
    "        image_batch = tf.identity(image_batch, 'input')\n",
    "        labels_batch = tf.identity(labels_batch, 'label_batch')\n",
    "\n",
    "        # Build the inference graph\n",
    "        prelogits, _ = network.inference(image_batch, args.keep_probability, \n",
    "            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size,\n",
    "            weight_decay=args.weight_decay)\n",
    "        \n",
    "        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name='embeddings')\n",
    "        # Split embeddings into anchor, positive and negative and calculate triplet loss\n",
    "        anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\n",
    "        triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\n",
    "        \n",
    "        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n",
    "            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "        # Calculate the total losses\n",
    "        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        total_loss = tf.add_n([triplet_loss] + regularization_losses, name='total_loss')\n",
    "\n",
    "        # Build a Graph that trains the model with one batch of examples and updates the model parameters\n",
    "        train_op = facenet.train(total_loss, global_step, args.optimizer, \n",
    "            learning_rate, args.moving_average_decay, tf.global_variables())\n",
    "        \n",
    "        # Create a saver\n",
    "        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n",
    "\n",
    "        # Build the summary operation based on the TF collection of Summaries.\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        # Start running operations on the Graph.\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \n",
    "\n",
    "        # Initialize variables\n",
    "        sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\n",
    "        sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\n",
    "\n",
    "        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "        coord = tf.train.Coordinator()\n",
    "        tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "        with sess.as_default():\n",
    "\n",
    "            if args.pretrained_model:\n",
    "                print('Restoring pretrained model: %s' % args.pretrained_model)\n",
    "                saver.restore(sess, os.path.expanduser(args.pretrained_model))\n",
    "\n",
    "            # Training and validation loop\n",
    "            epoch = 0\n",
    "            while epoch < args.max_nrof_epochs:\n",
    "                step = sess.run(global_step, feed_dict=None)\n",
    "                epoch = step // args.epoch_size\n",
    "                # Train for one epoch\n",
    "                train(args, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n",
    "                    batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n",
    "                    embeddings, total_loss, train_op, summary_op, summary_writer, args.learning_rate_schedule_file,\n",
    "                    args.embedding_size, anchor, positive, negative, triplet_loss)\n",
    "\n",
    "                # Save variables and the metagraph if it doesn't exist already\n",
    "                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)\n",
    "\n",
    "                # Evaluate on LFW\n",
    "                if args.lfw_dir:\n",
    "                    evaluate(sess, lfw_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n",
    "                            batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, args.batch_size, \n",
    "                            args.lfw_nrof_folds, log_dir, step, summary_writer, args.embedding_size)\n",
    "\n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n",
    "          batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n",
    "          embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file,\n",
    "          embedding_size, anchor, positive, negative, triplet_loss):\n",
    "    batch_number = 0\n",
    "    \n",
    "    if args.learning_rate>0.0:\n",
    "        lr = args.learning_rate\n",
    "    else:\n",
    "        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\n",
    "    while batch_number < args.epoch_size:\n",
    "        # Sample people randomly from the dataset\n",
    "        image_paths, num_per_class = sample_people(dataset, args.people_per_batch, args.images_per_person)\n",
    "        \n",
    "        print('Running forward pass on sampled images: ', end='')\n",
    "        start_time = time.time()\n",
    "        nrof_examples = args.people_per_batch * args.images_per_person\n",
    "        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\n",
    "        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n",
    "        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n",
    "        emb_array = np.zeros((nrof_examples, embedding_size))\n",
    "        nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\n",
    "        for i in range(nrof_batches):\n",
    "            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n",
    "            emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size, \n",
    "                learning_rate_placeholder: lr, phase_train_placeholder: True})\n",
    "            emb_array[lab,:] = emb\n",
    "        print('%.3f' % (time.time()-start_time))\n",
    "\n",
    "        # Select triplets based on the embeddings\n",
    "        print('Selecting suitable triplets for training')\n",
    "        triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, \n",
    "            image_paths, args.people_per_batch, args.alpha)\n",
    "        selection_time = time.time() - start_time\n",
    "        print('(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds' % \n",
    "            (nrof_random_negs, nrof_triplets, selection_time))\n",
    "\n",
    "        # Perform training on the selected triplets\n",
    "        nrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\n",
    "        triplet_paths = list(itertools.chain(*triplets))\n",
    "        labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\n",
    "        triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\n",
    "        sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\n",
    "        nrof_examples = len(triplet_paths)\n",
    "        train_time = 0\n",
    "        i = 0\n",
    "        emb_array = np.zeros((nrof_examples, embedding_size))\n",
    "        loss_array = np.zeros((nrof_triplets,))\n",
    "        summary = tf.Summary()\n",
    "        step = 0\n",
    "        while i < nrof_batches:\n",
    "            start_time = time.time()\n",
    "            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n",
    "            feed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}\n",
    "            err, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict)\n",
    "            emb_array[lab,:] = emb\n",
    "            loss_array[i] = err\n",
    "            duration = time.time() - start_time\n",
    "            print('Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f' %\n",
    "                  (epoch, batch_number+1, args.epoch_size, duration, err))\n",
    "            batch_number += 1\n",
    "            i += 1\n",
    "            train_time += duration\n",
    "            summary.value.add(tag='loss', simple_value=err)\n",
    "            \n",
    "        # Add validation loss and accuracy to summary\n",
    "        #pylint: disable=maybe-no-member\n",
    "        summary.value.add(tag='time/selection', simple_value=selection_time)\n",
    "        summary_writer.add_summary(summary, step)\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha):\n",
    "    \"\"\" Select the triplets for training\n",
    "    \"\"\"\n",
    "    trip_idx = 0\n",
    "    emb_start_idx = 0\n",
    "    num_trips = 0\n",
    "    triplets = []\n",
    "    \n",
    "    # VGG Face: Choosing good triplets is crucial and should strike a balance between\n",
    "    #  selecting informative (i.e. challenging) examples and swamping training with examples that\n",
    "    #  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\n",
    "    #  the image n at random, but only between the ones that violate the triplet loss margin. The\n",
    "    #  latter is a form of hard-negative mining, but it is not as aggressive (and much cheaper) than\n",
    "    #  choosing the maximally violating example, as often done in structured output learning.\n",
    "\n",
    "    for i in xrange(people_per_batch):\n",
    "        nrof_images = int(nrof_images_per_class[i])\n",
    "        for j in xrange(1,nrof_images):\n",
    "            a_idx = emb_start_idx + j - 1\n",
    "            neg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\n",
    "            for pair in xrange(j, nrof_images): # For every possible positive pair.\n",
    "                p_idx = emb_start_idx + pair\n",
    "                pos_dist_sqr = np.sum(np.square(embeddings[a_idx]-embeddings[p_idx]))\n",
    "                neg_dists_sqr[emb_start_idx:emb_start_idx+nrof_images] = np.NaN\n",
    "                #all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\n",
    "                all_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\n",
    "                nrof_random_negs = all_neg.shape[0]\n",
    "                if nrof_random_negs>0:\n",
    "                    rnd_idx = np.random.randint(nrof_random_negs)\n",
    "                    n_idx = all_neg[rnd_idx]\n",
    "                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\n",
    "                    #print('Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)' % \n",
    "                    #    (trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\n",
    "                    trip_idx += 1\n",
    "\n",
    "                num_trips += 1\n",
    "\n",
    "        emb_start_idx += nrof_images\n",
    "\n",
    "    np.random.shuffle(triplets)\n",
    "    return triplets, num_trips, len(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sample_people(dataset, people_per_batch, images_per_person):\n",
    "    nrof_images = people_per_batch * images_per_person\n",
    "  \n",
    "    # Sample classes from the dataset\n",
    "    nrof_classes = len(dataset)\n",
    "    class_indices = np.arange(nrof_classes)\n",
    "    np.random.shuffle(class_indices)\n",
    "    \n",
    "    i = 0\n",
    "    image_paths = []\n",
    "    num_per_class = []\n",
    "    sampled_class_indices = []\n",
    "    # Sample images from these classes until we have enough\n",
    "    while len(image_paths)<nrof_images:\n",
    "        class_index = class_indices[i]\n",
    "        nrof_images_in_class = len(dataset[class_index])\n",
    "        image_indices = np.arange(nrof_images_in_class)\n",
    "        np.random.shuffle(image_indices)\n",
    "        nrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images-len(image_paths))\n",
    "        idx = image_indices[0:nrof_images_from_class]\n",
    "        image_paths_for_class = [dataset[class_index].image_paths[j] for j in idx]\n",
    "        sampled_class_indices += [class_index]*nrof_images_from_class\n",
    "        image_paths += image_paths_for_class\n",
    "        num_per_class.append(nrof_images_from_class)\n",
    "        i+=1\n",
    "  \n",
    "    return image_paths, num_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n",
    "        batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, batch_size, \n",
    "        nrof_folds, log_dir, step, summary_writer, embedding_size):\n",
    "    start_time = time.time()\n",
    "    # Run forward pass to calculate embeddings\n",
    "    print('Running forward pass on LFW images: ', end='')\n",
    "    \n",
    "    nrof_images = len(actual_issame)*2\n",
    "    assert(len(image_paths)==nrof_images)\n",
    "    labels_array = np.reshape(np.arange(nrof_images),(-1,3))\n",
    "    image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n",
    "    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n",
    "    emb_array = np.zeros((nrof_images, embedding_size))\n",
    "    nrof_batches = int(np.ceil(nrof_images / batch_size))\n",
    "    label_check_array = np.zeros((nrof_images,))\n",
    "    for i in xrange(nrof_batches):\n",
    "        batch_size = min(nrof_images-i*batch_size, batch_size)\n",
    "        emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\n",
    "            learning_rate_placeholder: 0.0, phase_train_placeholder: False})\n",
    "        emb_array[lab,:] = emb\n",
    "        label_check_array[lab] = 1\n",
    "    print('%.3f' % (time.time()-start_time))\n",
    "    \n",
    "    assert(np.all(label_check_array==1))\n",
    "    \n",
    "    _, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds)\n",
    "    \n",
    "    print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "    print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))\n",
    "    lfw_time = time.time() - start_time\n",
    "    # Add validation loss and accuracy to summary\n",
    "    summary = tf.Summary()\n",
    "    #pylint: disable=maybe-no-member\n",
    "    summary.value.add(tag='lfw/accuracy', simple_value=np.mean(accuracy))\n",
    "    summary.value.add(tag='lfw/val_rate', simple_value=val)\n",
    "    summary.value.add(tag='time/lfw', simple_value=lfw_time)\n",
    "    summary_writer.add_summary(summary, step)\n",
    "    with open(os.path.join(log_dir,'lfw_result.txt'),'at') as f:\n",
    "        f.write('%d\\t%.5f\\t%.5f\\n' % (step, np.mean(accuracy), val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\n",
    "    # Save the model checkpoint\n",
    "    print('Saving variables')\n",
    "    start_time = time.time()\n",
    "    checkpoint_path = os.path.join(model_dir, 'model-%s.ckpt' % model_name)\n",
    "    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n",
    "    save_time_variables = time.time() - start_time\n",
    "    print('Variables saved in %.2f seconds' % save_time_variables)\n",
    "    metagraph_filename = os.path.join(model_dir, 'model-%s.meta' % model_name)\n",
    "    save_time_metagraph = 0  \n",
    "    if not os.path.exists(metagraph_filename):\n",
    "        print('Saving metagraph')\n",
    "        start_time = time.time()\n",
    "        saver.export_meta_graph(metagraph_filename)\n",
    "        save_time_metagraph = time.time() - start_time\n",
    "        print('Metagraph saved in %.2f seconds' % save_time_metagraph)\n",
    "    summary = tf.Summary()\n",
    "    #pylint: disable=maybe-no-member\n",
    "    summary.value.add(tag='time/save_variables', simple_value=save_time_variables)\n",
    "    summary.value.add(tag='time/save_metagraph', simple_value=save_time_metagraph)\n",
    "    summary_writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate_from_file(filename, epoch):\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split('#', 1)[0]\n",
    "            if line:\n",
    "                par = line.strip().split(':')\n",
    "                e = int(par[0])\n",
    "                lr = float(par[1])\n",
    "                if e <= epoch:\n",
    "                    learning_rate = lr\n",
    "                else:\n",
    "                    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments(argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--logs_base_dir', type=str, \n",
    "        help='Directory where to write event logs.', default='~/logs/facenet')\n",
    "    parser.add_argument('--models_base_dir', type=str,\n",
    "        help='Directory where to write trained models and checkpoints.', default='~/models/facenet')\n",
    "    parser.add_argument('--gpu_memory_fraction', type=float,\n",
    "        help='Upper bound on the amount of GPU memory that will be used by the process.', default=1.0)\n",
    "    parser.add_argument('--pretrained_model', type=str,\n",
    "        help='Load a pretrained model before training starts.')\n",
    "    parser.add_argument('--data_dir', type=str,\n",
    "        help='Path to the data directory containing aligned face patches.',\n",
    "        default='~/datasets/casia/casia_maxpy_mtcnnalign_182_160')\n",
    "    parser.add_argument('--model_def', type=str,\n",
    "        help='Model definition. Points to a module containing the definition of the inference graph.', default='models.inception_resnet_v1')\n",
    "    parser.add_argument('--max_nrof_epochs', type=int,\n",
    "        help='Number of epochs to run.', default=500)\n",
    "    parser.add_argument('--batch_size', type=int,\n",
    "        help='Number of images to process in a batch.', default=90)\n",
    "    parser.add_argument('--image_size', type=int,\n",
    "        help='Image size (height, width) in pixels.', default=160)\n",
    "    parser.add_argument('--people_per_batch', type=int,\n",
    "        help='Number of people per batch.', default=45)\n",
    "    parser.add_argument('--images_per_person', type=int,\n",
    "        help='Number of images per person.', default=40)\n",
    "    parser.add_argument('--epoch_size', type=int,\n",
    "        help='Number of batches per epoch.', default=1000)\n",
    "    parser.add_argument('--alpha', type=float,\n",
    "        help='Positive to negative triplet distance margin.', default=0.2)\n",
    "    parser.add_argument('--embedding_size', type=int,\n",
    "        help='Dimensionality of the embedding.', default=128)\n",
    "    parser.add_argument('--random_crop', \n",
    "        help='Performs random cropping of training images. If false, the center image_size pixels from the training images are used. ' +\n",
    "         'If the size of the images in the data directory is equal to image_size no cropping is performed', action='store_true')\n",
    "    parser.add_argument('--random_flip', \n",
    "        help='Performs random horizontal flipping of training images.', action='store_true')\n",
    "    parser.add_argument('--keep_probability', type=float,\n",
    "        help='Keep probability of dropout for the fully connected layer(s).', default=1.0)\n",
    "    parser.add_argument('--weight_decay', type=float,\n",
    "        help='L2 weight regularization.', default=0.0)\n",
    "    parser.add_argument('--optimizer', type=str, choices=['ADAGRAD', 'ADADELTA', 'ADAM', 'RMSPROP', 'MOM'],\n",
    "        help='The optimization algorithm to use', default='ADAGRAD')\n",
    "    parser.add_argument('--learning_rate', type=float,\n",
    "        help='Initial learning rate. If set to a negative value a learning rate ' +\n",
    "        'schedule can be specified in the file \"learning_rate_schedule.txt\"', default=0.1)\n",
    "    parser.add_argument('--learning_rate_decay_epochs', type=int,\n",
    "        help='Number of epochs between learning rate decay.', default=100)\n",
    "    parser.add_argument('--learning_rate_decay_factor', type=float,\n",
    "        help='Learning rate decay factor.', default=1.0)\n",
    "    parser.add_argument('--moving_average_decay', type=float,\n",
    "        help='Exponential decay for tracking of training parameters.', default=0.9999)\n",
    "    parser.add_argument('--seed', type=int,\n",
    "        help='Random seed.', default=666)\n",
    "    parser.add_argument('--learning_rate_schedule_file', type=str,\n",
    "        help='File containing the learning rate schedule that is used when learning_rate is set to to -1.', default='data/learning_rate_schedule.txt')\n",
    "\n",
    "    # Parameters for validation on LFW\n",
    "    parser.add_argument('--lfw_pairs', type=str,\n",
    "        help='The file containing the pairs to use for validation.', default='data/pairs.txt')\n",
    "    parser.add_argument('--lfw_dir', type=str,\n",
    "        help='Path to the data directory containing aligned face patches.', default='')\n",
    "    parser.add_argument('--lfw_nrof_folds', type=int,\n",
    "        help='Number of folds to use for cross validation. Mainly used for testing.', default=10)\n",
    "    return parser.parse_args(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(parse_arguments(sys.argv[1:]))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
